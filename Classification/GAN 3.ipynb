{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b80afd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c2cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\trush\\OneDrive\\Documents\\WFU Grad School Info\\BAN 6025 Machine Learning\\Data\\iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fb04454",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['species'] = le.fit_transform(df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a38c568b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42909e3c",
   "metadata": {},
   "source": [
    "This is a Generative Adversarial Network (GAN). It is used to create synthetic records within a dataset. A GAN has two parts, a generator and a discriminator. The generator, generates the fake records and their class labels. The discriminator tries to prove that these fake records are fake by using the real records from the dataset. The model iterates, and the generator makes fake records until it can fool the discriminator. Those fake records can the be used in the dataset. \n",
    "\n",
    "This works especially well when using classification models. If I have a dataset with 70% of one class and only 30% of another class. I can use the GAN to make those missing 40% of the second class which will remove any bias in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9c0bc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot labels shape: (150, 3)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\trush\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_159', 'keras_tensor_160']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m labels_batch \u001b[38;5;241m=\u001b[39m labels_batch\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Train the discriminator\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch([real_batch, labels_batch], np\u001b[38;5;241m.\u001b[39mones((BATCH_SIZE, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    100\u001b[0m fake_loss \u001b[38;5;241m=\u001b[39m discriminator\u001b[38;5;241m.\u001b[39mtrain_on_batch([generated_batch, labels_batch], np\u001b[38;5;241m.\u001b[39mzeros((BATCH_SIZE, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m    101\u001b[0m discriminator_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (real_loss \u001b[38;5;241m+\u001b[39m fake_loss)\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:549\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 549\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(data())\n\u001b[0;32m    550\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    122\u001b[0m     one_step_on_data, args\u001b[38;5;241m=\u001b[39m(data,)\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    125\u001b[0m     outputs,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:61\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     53\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(\n\u001b[0;32m     55\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m     56\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m )\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(\n\u001b[0;32m     62\u001b[0m     loss, sample_weight\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mshape(tree\u001b[38;5;241m.\u001b[39mflatten(x)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mscale_loss(loss)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "Y = iris.target  # Labels\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to pandas DataFrame\n",
    "real_data = pd.DataFrame(X, columns=[str(i) for i in range(1, 5)])  # Four features\n",
    "real_labels = Y  # No need to convert to numpy array\n",
    "\n",
    "# One-hot encode labels\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels = one_hot_encoder.fit_transform(real_labels.reshape(-1, 1))\n",
    "print(\"One-hot labels shape:\", one_hot_labels.shape)\n",
    "\n",
    "# Constants\n",
    "NOISE_DIM = 100\n",
    "NUM_CLASSES = one_hot_labels.shape[1]  # Should be 3 for the Iris dataset\n",
    "NUM_FEATURES = X.shape[1]  # Should be 4 for the Iris dataset\n",
    "BATCH_SIZE = 32  # Adjusted to be smaller since Iris dataset is small\n",
    "TRAINING_STEPS = 5000\n",
    "\n",
    "# Generator\n",
    "def create_generator():\n",
    "    noise_input = Input(shape=(NOISE_DIM,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    merged_input = Concatenate()([noise_input, class_input])\n",
    "    hidden = Dense(128, activation='relu')(merged_input)\n",
    "    output = Dense(NUM_FEATURES, activation='linear')(hidden)\n",
    "    model = Model(inputs=[noise_input, class_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def create_discriminator():\n",
    "    data_input = Input(shape=(NUM_FEATURES,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    merged_input = Concatenate()([data_input, class_input])\n",
    "    hidden = Dense(128, activation='relu')(merged_input)\n",
    "    output = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=[data_input, class_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# cGAN\n",
    "def create_cgan(generator, discriminator):\n",
    "    noise_input = Input(shape=(NOISE_DIM,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    generated_data = generator([noise_input, class_input])\n",
    "    validity = discriminator([generated_data, class_input])\n",
    "    model = Model(inputs=[noise_input, class_input], outputs=validity)\n",
    "    return model\n",
    "\n",
    "# Create and compile the models\n",
    "discriminator = create_discriminator()\n",
    "generator = create_generator()\n",
    "gan = create_cgan(generator, discriminator)\n",
    "\n",
    "# Ensure that only the generator is trained during GAN training\n",
    "discriminator.trainable = False\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Generate instances for a given class\n",
    "def generate_data(generator, data_class, num_instances):\n",
    "    one_hot_class = one_hot_encoder.transform(np.array([[data_class]]))\n",
    "    noise = np.random.normal(0, 1, (num_instances, NOISE_DIM))\n",
    "    generated_data = generator.predict([noise, np.repeat(one_hot_class, num_instances, axis=0)])\n",
    "    return pd.DataFrame(generated_data, columns=[str(i) for i in range(1, 5)])  # Four features\n",
    "\n",
    "# Train GAN\n",
    "step_list, loss_list_discriminator, loss_list_generator = [], [], []\n",
    "\n",
    "for step in range(TRAINING_STEPS):\n",
    "    # Select a random batch of real data with labels\n",
    "    idx = np.random.randint(0, real_data.shape[0], BATCH_SIZE)\n",
    "    real_batch = real_data.iloc[idx].values\n",
    "    labels_batch = one_hot_labels[idx]\n",
    "\n",
    "    # Generate a batch of new data\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_DIM))\n",
    "    generated_batch = generator.predict([noise, labels_batch])\n",
    "\n",
    "    real_batch = real_batch.astype('float32')\n",
    "    labels_batch = labels_batch.astype('float32')\n",
    "    \n",
    "    # Train the discriminator\n",
    "    real_loss = discriminator.train_on_batch([real_batch, labels_batch], np.ones((BATCH_SIZE, 1)))\n",
    "    fake_loss = discriminator.train_on_batch([generated_batch, labels_batch], np.zeros((BATCH_SIZE, 1)))\n",
    "    discriminator_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "    # Train the generator\n",
    "    generator_loss = gan.train_on_batch([noise, labels_batch], np.ones((BATCH_SIZE, 1)))\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step: {step}, Discriminator Loss: {discriminator_loss:.4f}, Generator Loss: {generator_loss:.4f}\")\n",
    "\n",
    "    # Save losses for plotting later\n",
    "    step_list.append(step)\n",
    "    loss_list_discriminator.append(discriminator_loss)\n",
    "    loss_list_generator.append(generator_loss)\n",
    "\n",
    "    # Visualization every 500 steps\n",
    "    if step % 500 == 0:\n",
    "        generated_samples = generate_data(generator, 0, 10)  # Change class as needed (0, 1, or 2)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(generated_samples.iloc[:, 0], generated_samples.iloc[:, 1])  # Scatter plot for first two features\n",
    "        plt.title(f\"Generated Samples at Step {step}\")\n",
    "        plt.xlabel(\"Feature 1\")\n",
    "        plt.ylabel(\"Feature 2\")\n",
    "        plt.show()\n",
    "\n",
    "# Generate 40 instances of each class\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    generated_data = generate_data(generator, class_idx, 40)\n",
    "    print(f\"Generated data for class {class_idx}:\")\n",
    "    print(generated_data)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(step_list, loss_list_discriminator)\n",
    "plt.title('Discriminator Loss vs. Step')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(step_list, loss_list_generator)\n",
    "plt.title('Generator Loss vs. Step')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebd9ef57",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Label'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m Y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Normalize the data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\trush\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Label'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "X = df.drop(['Label'], axis=1)\n",
    "Y = df['Label']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to pandas DataFrame\n",
    "real_data = pd.DataFrame(X, columns=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14',\n",
    "                                    '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
    "                                    '28', '29', '30'])\n",
    "real_labels = Y\n",
    "\n",
    "# One hot encode labels\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels = one_hot_encoder.fit_transform(np.array(real_labels).reshape(-1, 1))\n",
    "print(\"One-hot labels shape:\", one_hot_labels.shape)\n",
    "print(one_hot_labels)\n",
    "\n",
    "# Constants\n",
    "NOISE_DIM = 100\n",
    "NUM_CLASSES = 2\n",
    "NUM_FEATURES = 30\n",
    "BATCH_SIZE = 64\n",
    "TRAINING_STEPS = 5000\n",
    "\n",
    "# Generator\n",
    "def create_generator():\n",
    "    noise_input = Input(shape=(NOISE_DIM,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    merged_input = Concatenate()([noise_input, class_input])\n",
    "    hidden = Dense(128, activation='relu')(merged_input)\n",
    "    output = Dense(NUM_FEATURES, activation='linear')(hidden)\n",
    "    model = Model(inputs=[noise_input, class_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def create_discriminator():\n",
    "    data_input = Input(shape=(NUM_FEATURES,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    merged_input = Concatenate()([data_input, class_input])\n",
    "    hidden = Dense(128, activation='relu')(merged_input)\n",
    "    output = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=[data_input, class_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# cGAN\n",
    "def create_cgan(generator, discriminator):\n",
    "    noise_input = Input(shape=(NOISE_DIM,))\n",
    "    class_input = Input(shape=(NUM_CLASSES,))\n",
    "    generated_data = generator([noise_input, class_input])\n",
    "    validity = discriminator([generated_data, class_input])\n",
    "    model = Model(inputs=[noise_input, class_input], outputs=validity)\n",
    "    return model\n",
    "\n",
    "# Create and compile the Discriminator\n",
    "discriminator = create_discriminator()\n",
    "\n",
    "# Create the Generator\n",
    "generator = create_generator()\n",
    "\n",
    "# Create the GAN\n",
    "gan = create_cgan(generator, discriminator)\n",
    "\n",
    "# Ensure that only the generator is trained\n",
    "discriminator.trainable = False\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Generate instances for a given class\n",
    "def generate_data(generator, data_class, num_instances):\n",
    "    one_hot_class = one_hot_encoder.transform(np.array([[data_class]]))\n",
    "    noise = np.random.normal(0, 1, (num_instances, NOISE_DIM))\n",
    "    generated_data = generator.predict([noise, np.repeat(one_hot_class, num_instances, axis=0)])\n",
    "    return pd.DataFrame(generated_data, columns=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14',\n",
    "                                    '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27',\n",
    "                                    '28', '29', '30'])\n",
    "# Train GAN\n",
    "step_list = []\n",
    "loss_list_discriminator = []\n",
    "loss_list_generator = []\n",
    "for step in range(TRAINING_STEPS):\n",
    "    # Select a random batch of real data with labels\n",
    "    idx = np.random.randint(0, real_data.shape[0], BATCH_SIZE)\n",
    "    real_batch = real_data.iloc[idx].values\n",
    "    labels_batch = one_hot_labels[idx]\n",
    "\n",
    "    print(\"Real batch shape:\", real_batch.shape)\n",
    "    print(\"Labels batch shape:\", labels_batch.shape)\n",
    "\n",
    "    # Check if the input structure matches\n",
    "    print(\"Inputs to discriminator:\", [real_batch, labels_batch])\n",
    "\n",
    "    # Generate a batch of new data\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, NOISE_DIM))\n",
    "    generated_batch = generator.predict([noise, labels_batch])\n",
    "\n",
    "    \n",
    "    # Train the discriminator\n",
    "    real_loss = discriminator.train_on_batch([real_batch, labels_batch], np.ones((BATCH_SIZE, 1)))\n",
    "    fake_loss = discriminator.train_on_batch([generated_batch, labels_batch], np.zeros((BATCH_SIZE, 1)))\n",
    "    discriminator_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "\n",
    "    # Train the generator\n",
    "    generator_loss = gan.train_on_batch([noise, labels_batch], np.ones((BATCH_SIZE, 1)))\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step: {step}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")\n",
    "\n",
    "    if step % 500 == 0:\n",
    "        generated_samples = generate_data(generator, 1, 10)  # Change class as needed\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(generated_samples.iloc[:, 0], generated_samples.iloc[:, 1])  # Change to relevant columns\n",
    "        plt.title(f\"Generated Samples at Step {step}\")\n",
    "        plt.show()\n",
    "    \n",
    "    step_list.append(step)\n",
    "    loss_list_discriminator.append(discriminator_loss)\n",
    "    loss_list_generator.append(generator_loss)\n",
    "\n",
    "\n",
    "# Generate 40 instances of class 1\n",
    "generated_data = generate_data(generator, 1, 40)\n",
    "print(generated_data)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(step_list, loss_list_discriminator)\n",
    "plt.title('Discriminator Loss vs. Step')\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(step_list, loss_list_generator)\n",
    "plt.title('Generator Loss vs. Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(step_list, loss_list_discriminator)\n",
    "plt.title('Discriminator Loss vs. Step')\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(step_list, loss_list_generator)\n",
    "plt.title('Generator Loss vs. Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_class_1 = generate_data(generator, 1, 12226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e62ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = synthetic_data_class_1\n",
    "synthetic_data = pd.DataFrame(scaler.inverse_transform(synthetic_data), \n",
    "                              columns=['Dport', 'SrcBytes', 'DstBytes', 'SrcLoad', 'DstLoad', 'SIntPkt',\n",
    "       'DIntPkt', 'SIntPktAct', 'SrcJitter', 'DstJitter', 'sMaxPktSz',\n",
    "       'dMaxPktSz', 'sMinPktSz', 'Dur', 'TotPkts', 'TotBytes', 'Load', 'Loss',\n",
    "       'pLoss', 'pSrcLoss', 'pDstLoss', 'Rate', 'Temp', 'SpO2', 'Pulse_Rate',\n",
    "       'SYS', 'DIA', 'Heart_rate', 'Resp_Rate', 'ST'])\n",
    "\n",
    "synthetic_labels = [1]*12226\n",
    "synthetic_data['Label'] = synthetic_labels\n",
    "synthetic_data.to_csv('synthetic_attack_csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6272894",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.read_csv('synthetic_attack_csv.csv')\n",
    "synthetic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d06a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([df, synthetic_data], ignore_index=True)\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a117aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running a Classifier on the combined dataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "\n",
    "X = combined_data.drop('Label', axis=1)\n",
    "y = combined_data['Label']\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "print('The Accuracy of this model is: ', accuracy_score(y, y_pred))\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25085b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
